{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## LONG SHORT TERM MEMORY (LSTM) implementation using python ."
      ],
      "metadata": {
        "id": "TAlEolsYCzPd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ykBDRTC2Cp9Y"
      },
      "outputs": [],
      "source": [
        "import math \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler \n",
        "from keras.models import Sequential \n",
        "from keras.layers import Dense, LSTM \n",
        "import matplotlib.pyplot as plt "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# creating 2D feature Numpy array with random integers \n",
        "features = (np.random.randint(10,size=(100,1)))\n",
        "features.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZS9c0LZrDmGo",
        "outputId": "12e0731d-8d0b-473c-eaac-05759fbbc714"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#spliting dataset into 75 for train and 25 for test\n",
        "training_dataset=math.ceil(len(features)*.75)\n",
        "training_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gMMEh6g3D6ZG",
        "outputId": "f37536eb-2ba0-4046-bf0c-73d7d7fd9774"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "75"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocess the data , feature scaling to scale the data to be valued between 0 and 1,\n",
        "# which is a good practice to scale the data before feeding it into a neural network for optimal performance \n",
        "\n",
        "# scale all the data to be valued between 0 ad 1 \n",
        "scaler = MinMaxScaler(feature_range=(0,1))\n",
        "scaled_data=scaler.fit_transform(features)"
      ],
      "metadata": {
        "id": "kYS74WWCEN5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data=scaled_data[0:training_dataset,:]\n",
        "X_train=[]\n",
        "Y_train=[]\n",
        "for i in range(10,len(train_data)):\n",
        "  X_train.append(train_data[i-10:i,0])\n",
        "  Y_train.append(train_data[i,0])"
      ],
      "metadata": {
        "id": "AVLU0QVaFUhL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#converting the x_train and y_train into Numpy array values and\n",
        "#reshaping it into a 3D array , shape accepted by the LSTM model \n",
        "X_train,Y_train = np.array(X_train),np.array(Y_train)\n",
        "X_train=np.reshape(X_train,(X_train.shape[0],X_train.shape[1],1))"
      ],
      "metadata": {
        "id": "ms1F3Ey3F5mO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BUILDING THE ARCHITECTURE : \n",
        "1 - MAKE AN OBJECT OF THE SEQUENTIAL MODEL . THEN ADD THE LSTM LAYER WITH PARAMETERS (UNITS: THE DIMENSION OF PUTPUT SPACE, INPUT_SHAPE: THE SHAPE OF THE TRAINING SET,RETURN_SEQUENCES : True or False , determines whether to return the final output int the output sequence or the entire sequence)\n",
        "2 - We add 4 LSTM Layers, each with a dropout layer of values(0,2).\n",
        "dropout layer is a regularization technique used to prevent overfitting , but it may also increase training time in some cases . \n",
        "3 - the final layer is the output layer, a fully connected dense layer (units=1) as we are predicting only one value . \n",
        "dense layer performs the operation on the input layers and returns the output, every neuron in the previous layer is connected to the neurons in the next layer , hence it is called the fully connected dense layer . \n"
      ],
      "metadata": {
        "id": "DY-GSfybHgMN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Dropout \n",
        "# initialising the RNN\n",
        "model =  Sequential()\n",
        "model.add(LSTM(units=50,return_sequences=True,input_shape=(X_train.shape[1],1)))\n",
        "model.add(Dropout(0.2))\n",
        "# adding a second LSTM layer and dropout Layer \n",
        "model.add(LSTM(units=50,return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "# adding a third LSTM layer and dropout layer \n",
        "model.add(LSTM(units=50,return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "# adding a fourth LSTM layer and dropout layer \n",
        "model.add(LSTM(units=50))\n",
        "model.add(Dropout(0.2))\n",
        "# adding the output layer \n",
        "# for full connection layer we use dense \n",
        "# as the output is 1d se we use unit = 1\n",
        "model.add(Dense(units=1))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjuND8ubHfBa",
        "outputId": "a5012311-a1a3-42a6-9ce2-1d616ead9e92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.engine.sequential.Sequential at 0x7f665c12af50>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compiling the model using adam optimizer (A learning rate optimization algorithm used while training DNN model). \n",
        "Error is calculated by the loss function (mean squared error) as it is a regression problem, so we use the mean squared error loss function. \n",
        "\n",
        "the fit the model on 30 epoch(epochs are the number of times we pass the data into neural network) and a batch size of 50(we pass the data in bacthes , segmenting the data into smaller parts so as for network to process the data in parts). \n"
      ],
      "metadata": {
        "id": "W3yweSXfLfP9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',loss='mean_squared_error')\n",
        "model.fit(X_train,Y_train,epochs=30,batch_size=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lr6irzQnMYBB",
        "outputId": "751d7e0b-8637-4d38-e036-244bf17c8e2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "2/2 [==============================] - 8s 30ms/step - loss: 0.3260\n",
            "Epoch 2/30\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.2717\n",
            "Epoch 3/30\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.2148\n",
            "Epoch 4/30\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.1512\n",
            "Epoch 5/30\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.1043\n",
            "Epoch 6/30\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.1330\n",
            "Epoch 7/30\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.1443\n",
            "Epoch 8/30\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.1231\n",
            "Epoch 9/30\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.0966\n",
            "Epoch 10/30\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.1207\n",
            "Epoch 11/30\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.1206\n",
            "Epoch 12/30\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.1309\n",
            "Epoch 13/30\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.1229\n",
            "Epoch 14/30\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.1126\n",
            "Epoch 15/30\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.1126\n",
            "Epoch 16/30\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.1058\n",
            "Epoch 17/30\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 0.1099\n",
            "Epoch 18/30\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.1116\n",
            "Epoch 19/30\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.1078\n",
            "Epoch 20/30\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.1116\n",
            "Epoch 21/30\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.1051\n",
            "Epoch 22/30\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.1074\n",
            "Epoch 23/30\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 0.1050\n",
            "Epoch 24/30\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.1103\n",
            "Epoch 25/30\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.1034\n",
            "Epoch 26/30\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.1044\n",
            "Epoch 27/30\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 0.1064\n",
            "Epoch 28/30\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.1072\n",
            "Epoch 29/30\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 0.1038\n",
            "Epoch 30/30\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.1054\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f665bdbfd50>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# creating test data similar to train data , convert to Numpy array and reshape\n",
        "#Test data set \n",
        "test_data= scaled_data[training_dataset-10:,:]\n",
        "x_test = [] \n",
        "y_test = features[training_dataset:,:]\n",
        "for i in range(10,len(test_data)):\n",
        "  x_test.append(test_data[i-10:i,0])\n",
        "\n",
        "# convert x_test to a numpy array \n",
        "x_test = np.array(x_test)\n",
        "#reshape the data into 3_d array \n",
        "x_test=np.reshape(x_test,(x_test.shape[0],x_test.shape[1],1))\n"
      ],
      "metadata": {
        "id": "5BL9vj0dM5RG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# making the predictions and calculating the rmse score (smaller the rsme score,\n",
        "# better the model has performed)\n",
        "predictions=model.predict(x_test)\n",
        "#undo scaling \n",
        "predictions = scaler.inverse_transform(predictions)\n",
        "rmse=np.sqrt(np.mean(((predictions-y_test)**2)))\n",
        "rmse"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B1W2jyb8N4hd",
        "outputId": "8159ed53-a07c-4481-e5ea-c6feb1018536"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.465949528474709"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "_we now know what time series forecasting is and how to deal with time series data. \n",
        "_we now understand the structure of recurrent neural networks , how it differs from generic neural networks and the long term dependency problem in rnn \n",
        "_we don't use RNN for time_series forecasting because of  vanishing gradient problems in RNN \n",
        "_ understanding the LSTM structure: Structure of a single lstm cell \n",
        "_working on each of the gates of th LSTM and how to train the LSTM model . "
      ],
      "metadata": {
        "id": "hWjlf-bzO73M"
      }
    }
  ]
}